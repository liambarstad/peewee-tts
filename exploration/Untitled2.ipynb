{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657424f-9d09-4f32-8f26-1a19f72b081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.speaker_audio_dataset import SpeakerAudioDataset\n",
    "from model.layers.lstmp import LSTMPCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9d1a5c6-ce3f-4604-877f-34c30e15beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_params = {\n",
    "    'input_size': 80,\n",
    "    'hidden_size': 257,\n",
    "    'projection_size': 256,\n",
    "    'embedding_size': 256,\n",
    "    'num_layers': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab21e56c-9605-40b3-972f-8209f48b26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeakerAudioDataset('../data/utterance_corpuses/LibriTTS/dev-clean', sample_rate, mel_params)\n",
    "test_Y, test_X = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67437b2e-1592-4fc0-bce0-cd98fe8b790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpeakerAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate, mel_params):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.mel_params = mel_params\n",
    "        self.utterances = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file[-3:] == 'wav':\n",
    "                    info = file.split('_')\n",
    "                    if len(info) == 4:\n",
    "                        self.utterances.append([\n",
    "                            info[0], info[1], info[2]+'_'+info[3]\n",
    "                        ])\n",
    " \n",
    "        # audio\n",
    "        # | speaker_id | chapter_id | utterance_id | frame_id | ... 80 | \n",
    "        \n",
    "        # text\n",
    "        # | speaker_id | chapter_id | utterance_id | char_id | char_embed |\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "        # give length of all samples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        utterance = self.utterances[idx]\n",
    "        y, _ = librosa.load(f'{self.root_dir}/{utterance[0]}/{utterance[1]}/{\"_\".join(utterance)}')\n",
    "        mel_spec = librosa.feature.melspectrogram(y, sr=self.sample_rate, **self.mel_params)\n",
    "        return utterance[0], mel_spec.swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c931894-e3c6-4de9-859d-411e886a381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as func\n",
    "\n",
    "class SpeakerVerificationLSTMEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 projection_size, \n",
    "                 embedding_size,\n",
    "                 num_layers\n",
    "                ):\n",
    "        super(SpeakerVerificationLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection_size = projection_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size, \n",
    "            self.hidden_size, \n",
    "            self.num_layers, \n",
    "            proj_size=self.projection_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.projection_size, \n",
    "            out_features=self.embedding_size\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (64, 636, 80)\n",
    "        \n",
    "        # lstm with projection\n",
    "        _, (hx, cx) = self.lstm(x)\n",
    "        \n",
    "        # linear layer w/ relu\n",
    "        x = self.relu(self.linear(hx[-1]))\n",
    "        \n",
    "        # l2 normalize\n",
    "        x = func.normalize(x, p=2, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "322f1fcd-edf0-428e-9ef6-2501ab269859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0318, 0.0099, 0.0000,  ..., 0.0507, 0.0000, 0.0421],\n",
       "         [0.0324, 0.0092, 0.0000,  ..., 0.0505, 0.0000, 0.0423],\n",
       "         [0.0307, 0.0107, 0.0000,  ..., 0.0530, 0.0000, 0.0432],\n",
       "         ...,\n",
       "         [0.0356, 0.0106, 0.0000,  ..., 0.0518, 0.0000, 0.0440],\n",
       "         [0.0327, 0.0093, 0.0000,  ..., 0.0526, 0.0000, 0.0429],\n",
       "         [0.0339, 0.0103, 0.0000,  ..., 0.0515, 0.0000, 0.0410]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " torch.Size([64, 256]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate (xj, (xk1, ...xkM))\n",
    "# if j = k (speakers), positive\n",
    "# else negative\n",
    "# generate pos/neg alternatively\n",
    "# compute l2 norm response from lstm\n",
    "# (ej, (ek1, ...ekM))\n",
    "# compute centroid of (ek1, ...ekM), ckM\n",
    "\n",
    "x = model(torch.randn(64, 636, 80))\n",
    "# compute centroids of each row\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcbc960-0b0f-47cd-9732-b813bedf2ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SpeakerAudioDataSet' from 'datasets' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeakerAudioDataSet\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataloaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeakerAudioDataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeakerVerificationLSTMEncoder\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SpeakerAudioDataSet' from 'datasets' (unknown location)"
     ]
    }
   ],
   "source": [
    "from datasets import SpeakerAudioDataSet\n",
    "from dataloaders import SpeakerAudioDataLoader\n",
    "from models import SpeakerVerificationLSTMEncoder\n",
    "from transforms import Mel_Spec, Clip_Shuffle\n",
    "import transforms.transform_utils\n",
    "\n",
    "sample_rate = 22050\n",
    "mel_params = {\n",
    "    'sample_rate': sample_rate,\n",
    "    'n_fft': int(1024 * (sample_rate / 16000)),\n",
    "    'hop_length': int(256 * (sample_rate / 16000)),\n",
    "    'win_length': int(1024 * (sample_rate / 16000)),\n",
    "    'n_mels': 80\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'N_speakers': 64,\n",
    "    'M_utterances': 10,\n",
    "    'sources': {\n",
    "        'LibriTTS': {\n",
    "            'version': 'dev-clean'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'input_size': 80,\n",
    "    'hidden_size': 257,\n",
    "    'projection_size': 256,\n",
    "    'embedding_size': 256,\n",
    "    'num_layers': 3\n",
    "}\n",
    "\n",
    "batch_size = train_params['N_speakers'] / train_params['M_utterances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e7ab244-82a2-4234-96a5-50b8bcc09cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1397870347.py, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[63], line 51\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i, (speaker, audio) in enumerate(dataloader):\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = SpeakerAudioDataset(\n",
    "        root='../data/utterance_corpuses',\n",
    "        transform=[\n",
    "            Mel_Spec(mel_params),\n",
    "            Clip_Shuffle(clip_params),\n",
    "            transform_utils.To_Tensor(),\n",
    "        ]\n",
    ")\n",
    "\n",
    "dataloader = SpeakerAudioDataLoader(\n",
    "    dataset, \n",
    "    train_params['N_speakers'], \n",
    "    train_params['M_utterances'], \n",
    "    train_params['sources'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# transforms ToTensor\n",
    "\n",
    "epochs = 3\n",
    "total_speakers, total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "model = SpeakerVerificationLSTMEncoder(**model_params)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "      \n",
    "    cks = torch.Tensor(256,                                                                                                                                 \n",
    "    [\n",
    "        torch.init_some_shit_here(embedding_dims)\n",
    "        for speaker in total_speakers\n",
    "    ]\n",
    "                                                                                                                                       \n",
    "    for i, (speaker, audio) in enumerate(dataloader):\n",
    "        embed = model(audio)\n",
    "                                                                                                                                       \n",
    "        \n",
    "        # get batch_size examples\n",
    "        # forward/backwards + update\n",
    "        # if i % 5 == 0 (ex):\n",
    "        #     print/visualize/progress\n",
    "        #     e.g. epoch, step, input_size, graph, convergence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661d6594-27d9-4107-bdfa-a0786a4d2644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/utterance_corpuses/LibriTTS/dev-clean'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.join('../data/utterance_corpuses', 'LibriTTS', 'dev-clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
